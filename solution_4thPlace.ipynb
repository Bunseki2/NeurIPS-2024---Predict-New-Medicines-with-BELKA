{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the copy from @Ah(https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data) and modified using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-07T13:55:13.051420Z",
     "iopub.status.busy": "2024-06-07T13:55:13.050788Z",
     "iopub.status.idle": "2024-06-07T13:55:20.437561Z",
     "shell.execute_reply": "2024-06-07T13:55:20.436415Z",
     "shell.execute_reply.started": "2024-06-07T13:55:13.051386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastparquet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-07T13:55:26.636653Z",
     "iopub.status.busy": "2024-06-07T13:55:26.636357Z",
     "iopub.status.idle": "2024-06-07T13:55:27.603961Z",
     "shell.execute_reply": "2024-06-07T13:55:27.603049Z",
     "shell.execute_reply.started": "2024-06-07T13:55:26.636627Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score as APS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T13:55:28.524081Z",
     "iopub.status.busy": "2024-06-07T13:55:28.523828Z",
     "iopub.status.idle": "2024-06-07T13:55:28.538703Z",
     "shell.execute_reply": "2024-06-07T13:55:28.538067Z",
     "shell.execute_reply.started": "2024-06-07T13:55:28.524060Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "\n",
    "    PREPROCESS = False\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 4096\n",
    "    LR = 1e-3\n",
    "    WD = 0.05\n",
    "\n",
    "    NBR_FOLDS = 15\n",
    "    SELECTED_FOLDS = [0]\n",
    "\n",
    "    SEED = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T13:56:07.330374Z",
     "iopub.status.busy": "2024-06-07T13:56:07.329886Z",
     "iopub.status.idle": "2024-06-07T13:56:15.697304Z",
     "shell.execute_reply": "2024-06-07T13:56:15.696514Z",
     "shell.execute_reply.started": "2024-06-07T13:56:07.330349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "Running on TPU\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\") # \"local\" for 1VM TPU\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU\")\n",
    "    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "except tf.errors.NotFoundError:\n",
    "    print(\"Not on TPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T23:19:11.015798Z",
     "iopub.status.busy": "2024-06-06T23:19:11.015496Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.PREPROCESS:\n",
    "    enc = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n",
    "           '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n",
    "           '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36}\n",
    "    train_raw = pd.read_parquet('/kaggle/input/leash-BELKA/train.parquet')\n",
    "    smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles'].values\n",
    "    assert (smiles!=train_raw[train_raw['protein_name']=='HSA']['molecule_smiles'].values).sum() == 0\n",
    "    assert (smiles!=train_raw[train_raw['protein_name']=='sEH']['molecule_smiles'].values).sum() == 0\n",
    "    def encode_smile(smile):\n",
    "        tmp = [enc[i] for i in smile]\n",
    "        tmp = tmp + [0]*(142-len(tmp))\n",
    "        return np.array(tmp).astype(np.uint8)\n",
    "\n",
    "    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n",
    "    smiles_enc = np.stack(smiles_enc)\n",
    "    train = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n",
    "    train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n",
    "    train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n",
    "    train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values\n",
    "    train.to_parquet('train_enc.parquet')\n",
    "\n",
    "    test_raw = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n",
    "    smiles = test_raw['molecule_smiles'].values\n",
    "\n",
    "    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n",
    "    smiles_enc = np.stack(smiles_enc)\n",
    "    test = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n",
    "    test.to_parquet('test_enc.parquet')\n",
    "\n",
    "else:\n",
    "    train = pd.read_parquet('/kaggle/input/belka-enc-dataset/train_enc.parquet')\n",
    "    test = pd.read_parquet('/kaggle/input/belka-enc-dataset/test_enc.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURE_COLUMNS = [i for i in train.columns if i not in [\"bind1\",\"bind2\",\"bind3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=3))])\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pipeline.fit_transform(train[NUM_FEATURE_COLUMNS].values)\n",
    "\n",
    "train['pca-one'] = pca_result[:,0].astype('float16')\n",
    "train['pca-two'] = pca_result[:,1].astype('float16')\n",
    "train['pca-three'] = pca_result[:,2].astype('float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_parquet('/kaggle/working/train_desc_pca.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result = pipeline.transform(test[NUM_FEATURE_COLUMNS].values)\n",
    "\n",
    "test['pca-one'] = pca_result[:,0].astype('float16')\n",
    "test['pca-two'] = pca_result[:,1].astype('float16')\n",
    "test['pca-three'] = pca_result[:,2].astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.to_parquet('/kaggle/working/test_desc_pca.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    with strategy.scope():\n",
    "        INP_LEN = 145\n",
    "        NUM_FILTERS = 32\n",
    "        hidden_dim = 128\n",
    "\n",
    "        inputs = tf.keras.layers.Input(shape=(INP_LEN,)) #, dtype='int32')\n",
    "        x = tf.keras.layers.Embedding(input_dim=36, output_dim=hidden_dim, input_length=INP_LEN, mask_zero = True)(inputs)\n",
    "        x = tf.keras.layers.Conv1D(filters=NUM_FILTERS, kernel_size=7,  activation='relu', padding='valid',  strides=1)(x)\n",
    "        x = tf.keras.layers.Conv1D(filters=NUM_FILTERS*2, kernel_size=7,  activation='relu', padding='valid',  strides=1)(x)\n",
    "        x = tf.keras.layers.Conv1D(filters=NUM_FILTERS*3, kernel_size=7,  activation='relu', padding='valid',  strides=1)(x)\n",
    "        x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "        outputs = tf.keras.layers.Dense(3, activation='sigmoid')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=CFG.LR, weight_decay = CFG.WD)\n",
    "        loss = 'binary_crossentropy'\n",
    "        weighted_metrics = [tf.keras.metrics.AUC(curve='PR', name = 'avg_precision')]\n",
    "        model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURES = [f'enc{i}' for i in range(142)]\n",
    "FEATURES = [i for i in train.columns if i not in [\"bind1\",\"bind2\",\"bind3\"]]\n",
    "TARGETS = ['bind1', 'bind2', 'bind3']\n",
    "skf = StratifiedKFold(n_splits = CFG.NBR_FOLDS, shuffle = True, random_state = 42)\n",
    "\n",
    "all_preds = []\n",
    "for fold,(train_idx, valid_idx) in enumerate(skf.split(train, train[TARGETS].sum(1))):\n",
    "    \n",
    "    if fold not in CFG.SELECTED_FOLDS:\n",
    "        continue;\n",
    "    \n",
    "    X_train = train.loc[train_idx, FEATURES]\n",
    "    y_train = train.loc[train_idx, TARGETS]\n",
    "    X_val = train.loc[valid_idx, FEATURES]\n",
    "    y_val = train.loc[valid_idx, TARGETS]\n",
    "    \n",
    "    #xgb_model = XGBClassifier(use_label_encoder=False, n_estimators=500, random_state=43, )\n",
    "    #xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\", mode='min', verbose=1)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', filepath=f\"model-{fold}.h5\",\n",
    "                                                        save_best_only=True, save_weights_only=True,\n",
    "                                                    mode='min')\n",
    "    reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, verbose=1)\n",
    "    model = my_model()\n",
    "    \n",
    "    \n",
    "    history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=CFG.EPOCHS,\n",
    "            callbacks=[checkpoint, reduce_lr_loss, es],\n",
    "            batch_size=CFG.BATCH_SIZE,\n",
    "            verbose=1,\n",
    "        )\n",
    "    model.load_weights(f\"model-{fold}.h5\")\n",
    "    oof = model.predict(X_val, batch_size = 2*CFG.BATCH_SIZE)\n",
    "    print('fold :', fold, 'CV score =', APS(y_val, oof, average = 'micro'))\n",
    "    model.save(f\"/kaggle/working/my_model-{fold}.keras\") \n",
    "    \n",
    "    preds = model.predict(test, batch_size = 2*CFG.BATCH_SIZE)\n",
    "    all_preds.append(preds)\n",
    "\n",
    "preds = np.mean(all_preds, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = pd.read_parquet('/kaggle/input/test.parquet')\n",
    "tst['binds'] = 0\n",
    "tst.loc[tst['protein_name']=='BRD4', 'binds'] = preds[(tst['protein_name']=='BRD4').values, 0]\n",
    "tst.loc[tst['protein_name']=='HSA', 'binds'] = preds[(tst['protein_name']=='HSA').values, 1]\n",
    "tst.loc[tst['protein_name']=='sEH', 'binds'] = preds[(tst['protein_name']=='sEH').values, 2]\n",
    "tst[['id', 'binds']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "This Notebook has been released under the MIT open source license.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 8006601,
     "sourceId": 67356,
     "sourceType": "competition"
    },
    {
     "datasetId": 4914065,
     "sourceId": 8275617,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 179610695,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30514,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
